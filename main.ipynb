{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-22T06:19:11.390702Z",
     "start_time": "2025-02-22T06:19:10.046648Z"
    }
   },
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ticker = \"AAPL\"\n",
    "df = yf.download(ticker, period=\"8d\", interval=\"1m\")\n",
    "df = df[['Close', 'Volume']].dropna()\n",
    "data = df.to_numpy()\n",
    "\n",
    "sequence_length = 60\n",
    "\n",
    "X, y_data = [], []\n",
    "for i in range(len(data) - sequence_length):\n",
    "    X.append(data[i:i + sequence_length])\n",
    "    y_data.append(data[i + 1:i + sequence_length + 1])\n",
    "X = np.array(X)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "print(\"Sliding window input shape:\", X.shape)\n",
    "print(\"Sliding window output shape:\", y_data.shape)\n",
    "\n",
    "test_size = 360\n",
    "train_X = X[:-test_size]\n",
    "train_y = y_data[:-test_size]\n",
    "test_X = X[-test_size:]\n",
    "test_y = y_data[-test_size:]\n",
    "\n",
    "# Convert to tensors and immediately move them to GPU\n",
    "train_X_tensor = torch.tensor(train_X, dtype=torch.float32, device=device)\n",
    "train_y_tensor = torch.tensor(train_y, dtype=torch.float32, device=device)\n",
    "test_X_tensor = torch.tensor(test_X, dtype=torch.float32, device=device)\n",
    "test_y_tensor = torch.tensor(test_y, dtype=torch.float32, device=device)\n",
    "\n",
    "# Create datasets using the GPU-resident tensors\n",
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "test_dataset = TensorDataset(test_X_tensor, test_y_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "# Use num_workers=0 to avoid issues with GPU memory sharing between subprocesses.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliding window input shape: (3060, 60, 2)\n",
      "Sliding window output shape: (3060, 60, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T06:19:11.400285Z",
     "start_time": "2025-02-22T06:19:11.393834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, mlp_hidden_dim: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_dim, embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        mlp_output = self.mlp(x)\n",
    "        x = x + self.dropout(mlp_output)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_attention_blocks: int, num_heads: int, embed_dim: int, mlp_hidden_dim: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_projection = nn.Linear(2, embed_dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_hidden_dim, dropout)\n",
    "            for _ in range(num_attention_blocks)\n",
    "        ])\n",
    "\n",
    "        self.output_projection = nn.Linear(embed_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        out = self.output_projection(x)\n",
    "        return out"
   ],
   "id": "381fbd47d4e6e38e",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T06:19:16.928629Z",
     "start_time": "2025-02-22T06:19:11.443806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Model(\n",
    "    num_attention_blocks=4,\n",
    "    num_heads=4,\n",
    "    embed_dim=128,\n",
    "    mlp_hidden_dim=512,\n",
    "    dropout=0.1\n",
    ").to(\"cuda\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} - Loss: {avg_loss:.4f}\")"
   ],
   "id": "d73ac34995638b9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 14951284902.6977\n",
      "Epoch 2/10 - Loss: 15125461813.5814\n",
      "Epoch 3/10 - Loss: 15008143705.3023\n",
      "Epoch 4/10 - Loss: 14927678404.4651\n",
      "Epoch 5/10 - Loss: 14961304052.0930\n",
      "Epoch 6/10 - Loss: 14979070666.4186\n",
      "Epoch 7/10 - Loss: 15123576379.5349\n",
      "Epoch 8/10 - Loss: 14922584409.3023\n",
      "Epoch 9/10 - Loss: 14813283339.9070\n",
      "Epoch 10/10 - Loss: 14933272314.0465\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
